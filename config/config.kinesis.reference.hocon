{
  "input": {
    # Sources currently supported are:
    # "kinesis" for reading records from a Kinesis stream
    # "stdin" for reading unencoded tab-separated events from stdin
    # If set to "stdin", JSON documents will not be sent to Elasticsearch
    # but will be written to stdout.
    # "nsq" for reading unencoded tab-separated events from NSQ
    "type": "kinesis"

    # Stream name for incoming data
    "streamName": "test-kinesis-stream"

    # Config for Kinesis
    # "LATEST": most recent data.
    # "TRIM_HORIZON": oldest available data.
    # "AT_TIMESTAMP": Start from the record at or after the specified timestamp
    # Note: This only affects the first run of this application on a stream.
    "initialPosition": "AT_TIMESTAMP"

    # Need to be specified when initial-position is "AT_TIMESTAMP".
    # Timestamp format need to be in "yyyy-MM-ddTHH:mm:ssZ".
    # Ex: "2017-05-17T10:00:00Z"
    # Note: Time need to specified in UTC.
    "initialTimestamp": "2020-07-17T10:00:00Z"

    # Optional maximum number of records to get from Kinesis per call to GetRecords
    # Default value 10000
    "maxRecords": 9999

    # Region where the Kinesis stream is located
    # This field is optional if it can be resolved with AWS region provider chain.
    # It checks places like env variables, system properties, AWS profile file.
    # https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/regions/providers/DefaultAwsRegionProviderChain.html
    "region": "eu-central-1"

    # Optional endpoint url configuration to override aws kinesis endpoints,
    # this can be used to specify local endpoints when using localstack
    "customEndpoint": "127.0.0.1"

    # Optional endpoint url configuration to override aws dyanomdb endpoints for Kinesis checkpoints lease table,
    # this can be used to specify local endpoints when using Localstack
    "dynamodbCustomEndpoint": "http://localhost:4569"

    # "appName" is used for a DynamoDB table to maintain stream state.
    # Default value "snowplow-elasticsearch-loader"
    "appName": "test-app-name"

    # Events are accumulated in a buffer before being sent to Elasticsearch.
    # These values are optional.
    # The buffer is emptied whenever:
    # - the combined size of the stored records exceeds byteLimit or
    # - the number of stored records exceeds recordLimit or
    # - the time in milliseconds since it was last emptied exceeds timeLimit
    "buffer": {
      "byteLimit": 999999 # Default value 1000000
      "recordLimit": 499  # Default value 500
      "timeLimit": 499    # Default value 500
    }
  }

  "output": {
    "good": {
      # Good sinks currently supported are:
      # "elasticsearch" for writing good records to Elasticsearch
      # "stdout" for writing good records to stdout
      # Default value "elasticsearch"
      "type": "elasticsearch"

      # Events are indexed using an Elasticsearch Client
      # - endpoint: the cluster endpoint
      # - port (optional, default value 9200): the port the cluster can be accessed on
      #   - for http this is usually 9200
      #   - for transport this is usually 9300
      # - username (optional, remove if not active): http basic auth username
      # - password (optional, remove if not active): http basic auth password
      # - shardDateFormat (optional, remove if not needed): formatting used for sharding good stream, i.e. _yyyy-MM-dd
      # - shardDateField (optional, if not specified derived_tstamp is used): timestamp field for sharding good stream
      # - indexTimeout (optional, default value 60000): the maximum time to wait in milliseconds for a single http transaction when indexing events
      # - maxTimeout (optional, default value 10000): the maximum time to wait in milliseconds between retries after load failures
      # - maxRetries (optional, default value 6): the maximum number of request attempts before giving up
      # - ssl (optional, default value false): if using the http client, whether to use ssl or not
      "client": {
        "endpoint": "localhost"
        "port": 9200
        "username": "es-user"
        "password": "es-pass"
        "shardDateFormat": "_yyyy-MM-dd"
        "shardDateField": "derived_tstamp"
        "indexTimeout": 59999
        "maxTimeout": 9999
        "maxRetries": 5
        "ssl": true
      }

      # When using the AWS ES service
      # - signing: if using the http client and the AWS ES service you can sign your requests
      #    http://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html
      # - region where the AWS ES service is located
      # These values are optional.
      "aws": {
        "signing": true # Default value false
        "region": "eu-central-1" # Default value empty string
      }

      "cluster": {
        # The Elasticsearch index name
        # Default value "good"
        "index": "good"
        # The Elasticsearch index type.
        # Index types are deprecated in ES >=7.x
        # Therefore, it shouldn't be set with ES >=7.x
        "documentType": "good-doc"
      }

      # Bulk request to Elasticsearch will be splitted to
      # chunks according given limits.
      # These values are optional.
      "chunk": {
        "byteLimit": 999999 # Default value is 1000000
        "recordLimit": 499  # Default value is 500
      }
    }
    "bad" {
      # Bad sinks currently supported are:
      # "kinesis" for writing bad records to Kinesis
      # "stderr" for writing bad records to stderr
      # "nsq" for writing bad records to NSQ
      # "none" for ignoring bad records
      "type": "kinesis"

      # Stream name for events which are rejected by Elasticsearch
      "streamName": "test-kinesis-bad-stream"

      # Region where the Kinesis stream is located
      # This field is optional if it can be resolved with AWS region provider chain.
      # It checks places like env variables, system properties, AWS profile file.
      # https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/regions/providers/DefaultAwsRegionProviderChain.html
      "region": "eu-central-1"

      # Optional endpoint url configuration to override aws kinesis endpoints,
      # this can be used to specify local endpoints when using localstack
      "customEndpoint": "127.0.0.1:7846"

      # Optional. Limits the number of events in a single PutRecords request.
      # Maximum allowed: 500
      "recordLimit": 500

      # Optional. Limits the number of bytes in a single PutRecords request,
      # including records and partition keys.
      # Maximum allowed: 5 MB
      "byteLimit": 5242880
    }
  }

  # "ENRICHED_EVENTS" for a stream of successfully enriched events
  # "BAD_ROWS" for a stream of bad events
  # "JSON" for writing plain json
  "purpose": "ENRICHED_EVENTS"

  # Optional section for tracking endpoints
  "monitoring": {
    "snowplow": {
      "collector": "localhost:14322"
      "appId": "test-app-id"
    }

    "metrics": {
      # Optional, cloudwatch metrics are enabled by default.
      "cloudWatch": false
    }
  }
}
