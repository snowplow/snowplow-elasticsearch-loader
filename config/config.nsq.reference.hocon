{
  "input": {
    # Sources currently supported are:
    # "kinesis" for reading records from a Kinesis stream
    # "stdin" for reading unencoded tab-separated events from stdin
    # If set to "stdin", JSON documents will not be sent to Elasticsearch
    # but will be written to stdout.
    # "nsq" for reading unencoded tab-separated events from NSQ
    "type": "nsq"

    # Stream name for incoming data
    "streamName": "test-nsq-stream"

    # Channel name for NSQ source
    # If more than one application reading from the same NSQ topic at the same time,
    # all of them must have unique channel name for getting all the data from the same topic
    "channelName": "test-nsq-channel-name"

    # Host name for nsqlookupd
    "nsqlookupdHost": "127.0.0.1"

    # HTTP port for nsqd
    "nsqlookupdPort": 34189

    # Events are accumulated in a buffer before being sent to Elasticsearch.
    # The buffer is emptied:
    # - whenever the number of stored records exceeds recordLimit
    # - whenever the combined of stored records exceeds byteLimit
    # - at regular intervals, controlled by timeLimit in milliseconds
    # This value is optional.
    "buffer": {
      "byteLimit": 999999 # Default value 1000000
      "recordLimit": 499 # Default value 500
      "timeLimit": 499   # Default value 500
    }
  }

  "output": {
    "good": {
      # Good sinks currently supported are:
      # "elasticsearch" for writing good records to Elasticsearch
      # "stdout" for writing good records to stdout
      # Default value "elasticsearch"
      "type": "elasticsearch"

      # Events are indexed using an Elasticsearch Client
      # - endpoint: the cluster endpoint
      # - port (optional, default value 9200): the port the cluster can be accessed on
      #   - for http this is usually 9200
      #   - for transport this is usually 9300
      # - username (optional, remove if not active): http basic auth username
      # - password (optional, remove if not active): http basic auth password
      # - shardDateFormat (optional, remove if not needed): formatting used for sharding good stream, i.e. _yyyy-MM-dd
      # - shardDateField (optional, if not specified derived_tstamp is used): timestamp field for sharding good stream
      # - indexTimeout (optional, default value 60000): the maximum time to wait in milliseconds for a single http transaction when indexing events
      # - maxTimeout (optional, default value 10000): the maximum time to wait in milliseconds between retries after load failures
      # - maxRetries (optional, default value 6): the maximum number of request attempts before giving up
      # - ssl (optional, default value false): if using the http client, whether to use ssl or not
      "client": {
        "endpoint": "localhost"
        "port": 9200
        "username": "es-user"
        "password": "es-pass"
        "shardDateFormat": "_yyyy-MM-dd"
        "shardDateField": "derived_tstamp"
        "indexTimeout": 59999
        "maxTimeout": 9999
        "maxRetries": 5
        "ssl": true
      }

      # When using the AWS ES service
      # - signing: if using the http client and the AWS ES service you can sign your requests
      #    http://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html
      # - region where the AWS ES service is located
      # These values are optional.
      "aws": {
        "signing": true # Default value false
        "region": "eu-central-1" # Default value empty string
      }

      "cluster": {
        # The Elasticsearch index name
        # Default value "good"
        "index": "good"
        # The Elasticsearch index type.
        # Index types are deprecated in ES >=7.x
        # Therefore, it shouldn't be set with ES >=7.x
        "documentType": "good-doc"
      }

      # Bulk request to Elasticsearch will be splitted to
      # chunks according given limits.
      # These values are optional.
      "chunk": {
        "byteLimit": 999999 # Default value is 1000000
        "recordLimit": 499  # Default value is 500
      }
    }
    "bad" {
      # Bad sinks currently supported are:
      # "kinesis" for writing bad records to Kinesis
      # "stderr" for writing bad records to stderr
      # "nsq" for writing bad records to NSQ
      # "none" for ignoring bad records
      "type": "nsq"

      # Stream name for events which are rejected by Elasticsearch
      "streamName": "test-nsq-bad-stream"

      # Host name for nsqd
      "nsqdHost": "127.0.0.1"
      # HTTP port for nsqd
      "nsqdPort": 24509
    }
  }

  # "ENRICHED_EVENTS" for a stream of successfully enriched events
  # "BAD_ROWS" for a stream of bad events
  # "JSON" for writing plain json
  "purpose": "ENRICHED_EVENTS"

  # Optional section for tracking endpoints
  "monitoring": {
    "snowplow": {
      "collector": "localhost:14322"
      "appId": "test-app-id"
    }

    "metrics": {
      # Optional, cloudwatch metrics are enabled by default.
      "cloudWatch": false
    }
  }
}
