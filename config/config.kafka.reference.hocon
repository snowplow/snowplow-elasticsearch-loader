{
  "input": {
    # Sources currently supported are:
    # "kinesis" for reading records from a Kinesis stream
    # "stdin" for reading unencoded tab-separated events from stdin
    # If set to "stdin", JSON documents will not be sent to Elasticsearch
    # but will be written to stdout.
    # "nsq" for reading unencoded tab-separated events from NSQ
    "type": "kafka"

    # Topic name for incoming data
    "topicName": "enriched-events"


    # Kafka brokers
    "brokers": "localhost:9092"

    # Events are accumulated in a buffer before being sent to Elasticsearch.
    # The buffer is emptied whenever the number of stored records exceeds recordLimit
    "buffer": {
      "recordLimit": 499 # Default value 500
    }
  }

  "output": {
    "good": {
      # Good sinks currently supported are:
      # "elasticsearch" for writing good records to Elasticsearch
      # "stdout" for writing good records to stdout
      # Default value "elasticsearch"
      "type": "elasticsearch"

      # Events are indexed using an Elasticsearch Client
      # - endpoint: the cluster endpoint
      # - port (optional, default value 9200): the port the cluster can be accessed on
      #   - for http this is usually 9200
      #   - for transport this is usually 9300
      # - username (optional, remove if not active): http basic auth username
      # - password (optional, remove if not active): http basic auth password
      # - shardDateFormat (optional, remove if not needed): formatting used for sharding good stream, i.e. _yyyy-MM-dd
      # - shardDateField (optional, if not specified derived_tstamp is used): timestamp field for sharding good stream
      # - maxTimeout: the maximum attempt time before a client restart
      # - maxRetries (optional, default value 6): the maximum number of request attempts before giving up
      # - ssl (optional, default value false): if using the http client, whether to use ssl or not
      "client": {
        "endpoint": "localhost"
        "port": 9200
        "username": "es-user"
        "password": "es-pass"
        "shardDateFormat": "_yyyy-MM-dd"
        "shardDateField": "derived_tstamp"
        "maxTimeout": 9999
        "maxRetries": 5
        "ssl": true
      }

      # When using the AWS ES service
      # - signing: if using the http client and the AWS ES service you can sign your requests
      #    http://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html
      # - region where the AWS ES service is located
      # These values are optional.
      "aws": {
        "signing": true # Default value false
        "region": "eu-central-1" # Default value empty string
      }

      "cluster": {
        # The Elasticsearch index name
        # Default value "good"
        "index": "good"
        # The Elasticsearch index type.
        # Index types are deprecated in ES >=7.x
        # Therefore, it shouldn't be set with ES >=7.x
        "documentType": "good-doc"
      }

      # Bulk request to Elasticsearch will be splitted to
      # chunks according given limits.
      # These values are optional.
      "chunk": {
        "byteLimit": 999999 # Default value is 1000000
        "recordLimit": 499  # Default value is 500
      }
    }
    "bad" {
      # Bad sinks currently supported are:
      # "kinesis" for writing bad records to Kinesis
      # "stderr" for writing bad records to stderr
      # "nsq" for writing bad records to NSQ
      # "kafka" for writing bad records to Kafka
      # "none" for ignoring bad records
      "type": "kafka"

      # Topic name for events which are rejected by Elasticsearch
      "topicName": "bad-events"

      # brokers name for kafka
      "brokers": "localhost:9092"

      # Events are accumulated in a buffer before being sent to Kafka.
      # The buffer is emptied whenever:
      # - the combined size of the stored records reaches byteLimit or
      # - the time in milliseconds since it was last emptied exceeds timeLimit when
      #   a new event enters the buffer
      buffer {
        byteLimit = 50000
        timeLimit = 5000
      }
    }
  }

  # "ENRICHED_EVENTS" for a stream of successfully enriched events
  # "BAD_ROWS" for a stream of bad events
  # "JSON" for writing plain json
  "purpose": "ENRICHED_EVENTS"

  # Optional section for tracking endpoints
  "monitoring": {
    "snowplow": {
      "collector": "localhost:14322"
      "appId": "test-app-id"
    }

    "metrics": {
      # Optional, cloudwatch metrics are enabled by default.
      "cloudWatch": false
    }
  }
}
