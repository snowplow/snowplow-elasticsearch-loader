# Copyright (c) 2014-2016 Snowplow Analytics Ltd. All rights reserved.
#
# This program is licensed to you under the Apache License Version 2.0, and
# you may not use this file except in compliance with the Apache License
# Version 2.0.  You may obtain a copy of the Apache License Version 2.0 at
# http://www.apache.org/licenses/LICENSE-2.0.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the Apache License Version 2.0 is distributed on an "AS
# IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.  See the Apache License Version 2.0 for the specific language
# governing permissions and limitations there under.

# This file (config.hocon.sample) contains a template with
# configuration options for the Elasticsearch Loader.

# Sources currently supported are:
# "kinesis" for reading records from a Kinesis stream
# "stdin" for reading unencoded tab-separated events from stdin
# If set to "stdin", JSON documents will not be sent to Elasticsearch
# but will be written to stdout.
# "NSQ" for reading unencoded tab-separated events from NSQ
source = {{elasticsearchInputType}}

# Where to write good and bad records
sink {
  # Sinks currently supported are:
  # "elasticsearch" for writing good records to Elasticsearch
  # "stdout" for writing good records to stdout
  # "NSQ" for writing good records to NSQ  
  good: {{elasticsearchGoodOutputDestination}}

  # Sinks currently supported are:
  # "kinesis" for writing bad records to Kinesis
  # "stderr" for writing bad records to stderr
  # "NSQ" for writing bad records to NSQ    
  # "none" for ignoring bad records
  bad: {{elasticsearchBadOutputDestination}}
}

# "good" for a stream of successfully enriched events
# "bad" for a stream of bad events
stream-type: {{kinesisInStreamType}}

# The following are used to authenticate for the Amazon Kinesis sink.
#
# If both are set to "default", the default provider chain is used
# (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
#
# If both are set to "iam", use AWS IAM Roles to provision credentials.
#
# If both are set to "env", use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
aws {
  access-key: iam
  secret-key: iam
}

# config for NSQ sink and NSQ source
NSQ {
  #Topic name for good event source
  good-source-topic: "{{sinkKinesisNsqSourceGoodTopicName}}"

  #Channel name for good event source
  good-source-channel: "{{sinkKinesisNsqSourceGoodChannelName}}"

  #Topic name for good event sink
  good-sink: "{{sinkKinesisNsqSinkGoodTopicName}}"

  #Topic name for bad event source
  bad-source-topic: "{{sinkKinesisNsqSourceBadTopicName}}"

  #Channel name for bad event source
  bad-source-channel: "{{sinkKinesisNsqSourceBadChannelName}}"

  #Topic name for bad event sink
  bad-sink: "{{sinkKinesisNsqSinkBadTopicName}}"
     
  #Host name for NSQ tools
  host: "{{NsqHost}}"

  #Port for nsqd
  port: "{{NsqdPort}}"

  #Port for nsqlookupd
  lookup-port: {{NsqlookupdPort}}
}

kinesis {

  in {
    stream-name: {{kinesisInStreamName}} # Kinesis stream name

    # "LATEST": most recent data.
    # "TRIM_HORIZON": oldest available data.
    # Note: This only affects the first run of this application on a stream.
    initial-position: TRIM_HORIZON

    # Maximum number of records to get from Kinesis per call to GetRecords
    maxRecords: 10000
  }

  out {
    # Stream for enriched events which are rejected by Elasticsearch
    stream-name: {{kinesisOutStreamName}}
  }

  # Region where the Kinesis stream is located
  region: {{kinesisRegion}}

  # "app-name" is used for a DynamoDB table to maintain stream state.
  # You can set it automatically using: "SnowplowElasticsearchSink-${sink.kinesis.in.stream-name}"
  app-name: {{kinesisAppName}}
}

elasticsearch {

  # Events are indexed using an Elasticsearch Client
  # - endpoint: the cluster endpoint
  # - port: the port the cluster can be accessed on
  #   - for http this is usually 9200
  #   - for transport this is usually 9300
  # - max-timeout: the maximum attempt time before a client restart
  # - ssl: if using the http client, whether to use ssl or not
  client {
    endpoint: {{elasticsearchEndpoint}}
    port: {{elasticsearchTransportPort}}
    max-timeout: {{elasticsearchMaxTimeout}}
    ssl: false
  }

  # When using the AWS ES service
  # - signing: if using the http client and the AWS ES service you can sign your requests
  #    http://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html
  # - region where the AWS ES service is located
  aws {
    signing: false
    region: {{elasticsearchRegion}}
  }

  # index: the Elasticsearch index name
  # type: the Elasticsearch index type
  cluster {
    name: {{elasticsearchClusterName}}
    index: {{elasticsearchIndex}}
    type: {{elasticsearchType}}
  }
}

# Events are accumulated in a buffer before being sent to Elasticsearch.
# Note: Buffering is not supported by NSQ; will be ignored
# The buffer is emptied whenever:
# - the combined size of the stored records exceeds byte-limit or
# - the number of stored records exceeds record-limit or
# - the time in milliseconds since it was last emptied exceeds time-limit
buffer {
  byte-limit: {{elasticsearchBufferByteThreshold}}
  record-limit: {{elasticsearchBufferRecordThreshold}}
  time-limit: {{elasticsearchBufferTimeThreshold}}
}

# Optional section for tracking endpoints
monitoring {
  snowplow {
    collector-uri: "{{collectorUri}}"
    collector-port: 80
    app-id: {{sinkKinesisAppName}}
    method: GET
  }
}
